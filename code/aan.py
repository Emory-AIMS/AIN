from utils.attack_utils import fgm
import deep_cae as dcae
import resnet
from utils.decorator import *
from dependency import *
    

class AAN:
    """
    Adversarial Attack Network
    """

    def __init__(self, data, label, low_bound, up_bound, attack_epsilon, is_training):
        self.data = data
        self.label = label
        self.output_low_bound = low_bound
        self.output_up_bound = up_bound
        self.attack_epsilon = attack_epsilon
        self.is_training = is_training

        with tf.variable_scope('autoencoder'):
            #self._autoencoder = bae.BasicAE()
            self._autoencoder = dcae.DeepCAE(output_low_bound=self.output_low_bound, 
                                             output_up_bound=self.output_up_bound
                                            )
            self._autoencoder_prediction = self.data + self._autoencoder.prediction(self.data, self.is_training)
        # Adv data generated by AE
        with tf.variable_scope('target') as scope:
            self._target_adv = resnet.resnet18()
            #self._target_adv = resnet.resnet18()
            self._target_adv_logits, self._target_adv_prediction = self._target_adv.prediction(self._autoencoder_prediction)
            self._target_adv_accuracy = self._target_adv.accuracy(self._target_adv_prediction, self.label)
        # Fake data generated by attacking algo
        with tf.variable_scope(scope, reuse=True):
            self._target_attack = resnet.resnet18()
            self.data_fake = fgm(self._target_attack.prediction, data, label, eps=self.attack_epsilon, iters=FLAGS.FGM_ITERS)
        
        with tf.variable_scope(scope, reuse=True):
            self._target_fake = resnet.resnet18()
            self._target_fake_logits, self._target_fake_prediction = self._target_fake.prediction(self.data_fake)
            self.label_fake = self.get_label(self._target_fake_prediction)
            self._target_fake_accuracy = self._target_fake.accuracy(self._target_fake_prediction, label)
        # Clean data
        with tf.variable_scope(scope, reuse=True):
            self._target = resnet.resnet18()
            self._target_logits, self._target_prediction = self._target.prediction(data)
            self._target_accuracy = self._target.accuracy(self._target_prediction, label)
            
        self.prediction


    def get_label(self, prediction):
        n_class = prediction.get_shape().as_list()[1]
        indices = tf.argmax(prediction, axis=1)
        return tf.one_hot(indices, n_class, on_value=1.0, off_value=0.0)


    def vectorize(self, x):
        return tf.reshape(x, [-1, FLAGS.IMAGE_ROWS * FLAGS.IMAGE_COLS * FLAGS.NUM_CHANNELS])
        

    @lazy_method
    def loss_x(self, beta):
        loss_betaX = beta

        y_pred = self.vectorize(self._autoencoder_prediction) # adv
        y_true = self.vectorize(self.data)
        """
        Lx is for the distance loss between examples and adv_examples;
        Ly is for the prediction loss between examples and adv_examples
        """
        max_dist = tf.reduce_max(tf.abs(y_pred-y_true))
        if FLAGS.NORM_TYPE == "L2":
            #Lx_dist = tf.reduce_mean(tf.sqrt(tf.reduce_sum((y_pred-y_true)**2, 1)))
            Lx_dist = tf.reduce_mean(tf.norm(y_pred-y_true, ord=2, axis=1))
        if FLAGS.NORM_TYPE == "L1":
            #Lx_dist = tf.reduce_mean(tf.reduce_sum(tf.abs(y_pred-y_true), 1))
            Lx_dist = tf.reduce_mean(tf.norm(y_pred-y_true, ord=1, axis=1))
        elif FLAGS.NORM_TYPE == "INF":
            #Lx_dist = tf.reduce_mean(tf.reduce_max(tf.abs(y_pred-y_true), 1))]
            Lx_dist = tf.reduce_mean(tf.norm(y_pred-y_true, ord=np.inf, axis=1))
            
            
        Lx = loss_betaX * Lx_dist

        tf.summary.scalar("Loss_x", Lx)
        tf.summary.scalar("Dist_x", Lx_dist)
        tf.summary.scalar("Max_pixel_dist", max_dist)
        return Lx, Lx_dist, max_dist, (y_pred, y_true)
    

    @lazy_method
    def loss_y(self, beta_l, beta_f, beta_c):
        def loss_y_from_least(): 
            y_least = tf.argmin(self._target_logits, axis=1, output_type=tf.int32)
            if FLAGS.LOSS_MODE_LEAST == "ENTRO": # Minimize the distance of prediction from least prob of true data
                label_least = tf.one_hot(y_least, FLAGS.NUM_CLASSES)
                Ly_dist = tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits_v2(labels = label_least, logits = self._target_adv_logits)
                    # -tf.reduce_sum(self.label_fake * tf.log(self._target_adv_prediction), 1)
                )
            elif FLAGS.LOSS_MODE_LEAST == "C&W": # Maximize the logits at the least prob position of true data
                mask1 = tf.one_hot(y_least, FLAGS.NUM_CLASSES, on_value=0.0, off_value=float('inf'))
                mask2 = tf.one_hot(y_least, FLAGS.NUM_CLASSES, on_value=float('inf'), off_value=0.0)
                adv_logits_at_y_least = tf.reduce_max(tf.subtract(self._target_adv_logits, mask1), axis=1)
                adv_logits_max_exclude_y_least = tf.reduce_max(tf.subtract(self._target_adv_logits, mask2), axis=1)
                Ly_dist = tf.reduce_mean(
                    tf.maximum(tf.subtract(adv_logits_max_exclude_y_least, adv_logits_at_y_least), -FLAGS.KAPPA_FOR_LEAST)
                )
            return beta_l * Ly_dist, Ly_dist


        def loss_y_from_fake(): 
            if FLAGS.LOSS_MODE_FAKE == "LOGITS": # Minimize the distance of logits from faked data
                Ly_dist = tf.reduce_mean(
                    # tf.sqrt(tf.reduce_sum(tf.square(self._target_adv_logits-self._target_fake_logits), 1))
                    tf.norm(self._target_adv_logits-self._target_fake_logits, ord=2, axis=1)
                )
            elif FLAGS.LOSS_MODE_FAKE == "PREDS": # Minimize the distance of prediction from faked data
                Ly_dist = tf.reduce_mean(
                    # tf.sqrt(tf.reduce_sum(tf.square(self._target_adv_prediction-self._target_fake_prediction), 1))
                    tf.norm(self._target_adv_prediction-self._target_fake_prediction, ord=2, axis=1)
                )
            elif FLAGS.LOSS_MODE_FAKE == "ENTRO": # Minimize the distance of prediction from faked data
                Ly_dist = tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.label_fake, logits = self._target_adv_logits)
                    # -tf.reduce_sum(self.label_fake * tf.log(self._target_adv_prediction), 1)
                )
            elif FLAGS.LOSS_MODE_FAKE == "C&W": # Maximize the logits at max prob position of faked data
                y_faked = tf.argmax(self._target_fake_logits, axis=1, output_type=tf.int32)
                mask1 = tf.one_hot(y_faked, FLAGS.NUM_CLASSES, on_value=0.0, off_value=float('inf'))
                mask2 = tf.one_hot(y_faked, FLAGS.NUM_CLASSES, on_value=float('inf'), off_value=0.0)
                adv_logits_at_y_faked = tf.reduce_max(tf.subtract(self._target_adv_logits, mask1), axis=1)
                adv_logits_max_exclude_y_faked = tf.reduce_max(tf.subtract(self._target_adv_logits, mask2), axis=1)
                Ly_dist = tf.reduce_mean(
                    tf.maximum(tf.subtract(adv_logits_max_exclude_y_faked, adv_logits_at_y_faked), -FLAGS.KAPPA_FOR_FAKE)
                )
            return beta_f * Ly_dist, Ly_dist
        
        def loss_y_from_clean(): 
            if FLAGS.LOSS_MODE_CLEAN == "LOGITS": # Maximize the distance of logits from clean data
                Ly_dist = -1.0 * tf.reduce_mean(
                    # tf.sqrt(tf.reduce_sum(tf.square(self._target_adv_logits-self._target_logits), 1))
                    tf.norm(self._target_adv_logits-self._target_logits, ord=2, axis=1)
                )
            elif FLAGS.LOSS_MODE_CLEAN == "PREDS": # Maximize the distance of prediction from clean data
                Ly_dist = -1.0 * tf.reduce_mean(
                    # tf.sqrt(tf.reduce_sum(tf.square(self._target_adv_prediction-self._target_prediction), 1))
                    tf.norm(self._target_adv_prediction-self._target_prediction, ord=2, axis=1)
                )
            elif FLAGS.LOSS_MODE_CLEAN == "ENTRO": # Maximize the distance of prediction from clean data
                Ly_dist = -1.0 * tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.label, logits = self._target_adv_logits)
                    # -tf.reduce_sum(self.label_fake * tf.log(self._target_adv_prediction), 1)
                )
            return beta_c * Ly_dist, Ly_dist

        Ly_least, Ly_dist_least = loss_y_from_least()
        Ly_fake, Ly_dist_fake = loss_y_from_fake()
        Ly_clean, Ly_dist_clean = loss_y_from_clean()
        Ly = Ly_least + Ly_fake + Ly_clean

        tf.summary.scalar("Loss_y_least", Ly_least)
        tf.summary.scalar("Dist_y_least", Ly_dist_least)
        tf.summary.scalar("Loss_y_fake", Ly_fake)
        tf.summary.scalar("Dist_y_fake", Ly_dist_fake)
        tf.summary.scalar("Loss_y_clean", Ly_clean)
        tf.summary.scalar("Dist_y_clean", Ly_dist_clean)
        return Ly, (Ly_least, Ly_fake, Ly_clean), (Ly_dist_least, Ly_dist_fake, Ly_dist_clean)
    

    @lazy_method
    def loss(self, partial_loss, loss_x, loss_y):
        loss = tf.cond(tf.equal(partial_loss, "LOSS_X"), lambda: loss_x, lambda: loss_y)
        opt_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "autoencoder")

        if FLAGS.REG_SCALE is not None:
            regularize = tf.contrib.layers.l2_regularizer(FLAGS.REG_SCALE)
            #print tf.GraphKeys.TRAINABLE_VARIABLES
            reg_term = sum([regularize(param) for param in opt_vars])
            loss += reg_term
            tf.summary.scalar("Regularization", reg_term)
        tf.summary.scalar("Total_loss", loss)
        return loss


    @lazy_method
    def optimization(self, loss):
        """
        decayed_learning_rate = learning_rate *
                        decay_rate ^ (global_step / decay_steps)
        learning rate decay with decay_rate per decay_steps
        """
        # use lobal step to keep track of our iterations
        global_step = tf.Variable(0, name="OPT_GLOBAL_STEP", trainable=False)
        # decay
        learning_rate = tf.train.exponential_decay(FLAGS.LEARNING_RATE, global_step, 
            FLAGS.LEARNING_DECAY_STEPS, FLAGS.LEARNING_DECAY_RATE)

        opt_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "autoencoder")

        momentum = 0.9
        """optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(
            loss, var_list=opt_vars)"""
        if FLAGS.OPT_TYPE == "NEST":
            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)
        elif FLAGS.OPT_TYPE == "MOME":
            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=False)
        elif FLAGS.OPT_TYPE == "ADAM":
            optimizer = tf.train.AdamOptimizer(learning_rate)
        grads_and_vars = optimizer.compute_gradients(loss, var_list=opt_vars)
        op = optimizer.apply_gradients(grads_and_vars)

        # tensorboard
        for g, v in grads_and_vars:
            # print v.name
            name = v.name.replace(":", "_")
            tf.summary.histogram(name+"_gradients", g)
        tf.summary.scalar("Learning_rate", learning_rate)
        return op

    @lazy_property
    def prediction(self):
        return self._autoencoder_prediction

    def tf_load(self, sess, spec=""):
        self._autoencoder.tf_load(sess, FLAGS.AE_PATH, spec=spec)
        self._target.tf_load(sess, FLAGS.RESNET18_PATH, 'model.ckpt-5865')

    def tf_save(self, sess, spec=""):
        self._autoencoder.tf_save(sess, FLAGS.AE_PATH, spec=spec)
